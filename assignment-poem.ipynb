{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9033729,"sourceType":"datasetVersion","datasetId":5445251}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-25T15:06:27.000129Z","iopub.execute_input":"2024-07-25T15:06:27.000424Z","iopub.status.idle":"2024-07-25T15:06:28.008763Z","shell.execute_reply.started":"2024-07-25T15:06:27.000391Z","shell.execute_reply":"2024-07-25T15:06:28.007826Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/poems-dataset/bahar_norm.txt\n/kaggle/input/poems-dataset/khaghani_norm.txt\n/kaggle/input/poems-dataset/bahaee_norm.txt\n/kaggle/input/poems-dataset/nezari_norm.txt\n/kaggle/input/poems-dataset/shahnematollah_norm.txt\n/kaggle/input/poems-dataset/feyz_norm.txt\n/kaggle/input/poems-dataset/sanaee_norm.txt\n/kaggle/input/poems-dataset/kamal_norm.txt\n/kaggle/input/poems-dataset/shereno.txt\n/kaggle/input/poems-dataset/hatef_norm.txt\n/kaggle/input/poems-dataset/iqbal_norm.txt\n/kaggle/input/poems-dataset/khajoo_norm.txt\n/kaggle/input/poems-dataset/saeb_norm.txt\n/kaggle/input/poems-dataset/shahriar_norm.txt\n/kaggle/input/poems-dataset/vahshi_norm.txt\n/kaggle/input/poems-dataset/naserkhosro_norm.txt\n/kaggle/input/poems-dataset/jami_norm.txt\n/kaggle/input/poems-dataset/anvari_norm.txt\n/kaggle/input/poems-dataset/rahi_norm.txt\n/kaggle/input/poems-dataset/ghaani_norm.txt\n/kaggle/input/poems-dataset/razi_norm.txt\n/kaggle/input/poems-dataset/ferdousi_norm.txt\n/kaggle/input/poems-dataset/ouhadi_norm.txt\n/kaggle/input/poems-dataset/hafez_norm.txt\n/kaggle/input/poems-dataset/parvin_norm.txt\n/kaggle/input/poems-dataset/obeyd_norm.txt\n/kaggle/input/poems-dataset/salman_norm.txt\n/kaggle/input/poems-dataset/zahir_norm.txt\n/kaggle/input/poems-dataset/saadi_norm.txt\n/kaggle/input/poems-dataset/gilani_norm.txt\n/kaggle/input/poems-dataset/helali_norm.txt\n/kaggle/input/poems-dataset/roodaki_norm.txt\n/kaggle/input/poems-dataset/seyf_norm.txt\n/kaggle/input/poems-dataset/eraghi_norm.txt\n/kaggle/input/poems-dataset/manoochehri_norm.txt\n/kaggle/input/poems-dataset/amir_norm.txt\n/kaggle/input/poems-dataset/asadi_norm.txt\n/kaggle/input/poems-dataset/orfi_norm.txt\n/kaggle/input/poems-dataset/attar_norm.txt\n/kaggle/input/poems-dataset/moulavi_norm.txt\n/kaggle/input/poems-dataset/asad_norm.txt\n/kaggle/input/poems-dataset/khayyam_norm.txt\n/kaggle/input/poems-dataset/bidel_norm.txt\n/kaggle/input/poems-dataset/abusaeed_norm.txt\n/kaggle/input/poems-dataset/onsori_norm.txt\n/kaggle/input/poems-dataset/farrokhi_norm.txt\n/kaggle/input/poems-dataset/shabestari_norm.txt\n/kaggle/input/poems-dataset/khosro_norm.txt\n/kaggle/input/poems-dataset/babaafzal_norm.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import (\n    AutoTokenizer,\n    TextDataset,\n    DataCollatorForLanguageModeling,\n    Trainer,\n    TrainingArguments,\n    AutoModelWithLMHead,\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T16:10:32.532766Z","iopub.execute_input":"2024-07-25T16:10:32.533148Z","iopub.status.idle":"2024-07-25T16:10:32.611860Z","shell.execute_reply.started":"2024-07-25T16:10:32.533118Z","shell.execute_reply":"2024-07-25T16:10:32.610895Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def load_dataset(train_path, test_path, tokenizer):\n    train_dataset = TextDataset(\n        tokenizer=tokenizer, file_path=train_path, block_size=256\n    )\n\n    test_dataset = TextDataset(tokenizer=tokenizer, file_path=test_path, block_size=256)\n\n    data_collator = DataCollatorForLanguageModeling(\n        tokenizer=tokenizer,\n        mlm=False,\n    )\n\n    return train_dataset, test_dataset, data_collator","metadata":{"execution":{"iopub.status.busy":"2024-07-25T16:10:55.681992Z","iopub.execute_input":"2024-07-25T16:10:55.682733Z","iopub.status.idle":"2024-07-25T16:10:55.691073Z","shell.execute_reply.started":"2024-07-25T16:10:55.682701Z","shell.execute_reply":"2024-07-25T16:10:55.689775Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import glob\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ntorch.manual_seed(1337)\n\n# Load the text files\ndirectory = '/kaggle/input/poems-dataset/'\ntext_files = glob.glob(os.path.join(directory, '*.txt'))\ntext = []\nfor file_path in text_files:\n    try:\n        with open(file_path, 'r', encoding='utf-8') as file:\n            content = file.read()\n            text.append(content)\n    except UnicodeDecodeError as e:\n        print(f\"Skipping file {file_path} due to encoding error: {e}\")\n\n# Concatenate all the text content into a single string\ntext = ''.join(text)\n\n# Create a character-level vocabulary\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n# Create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.85 * len(data))\ntrain_data = data[:n]\nval_data = data[n:]\n\n# Ensure block size is less than data size\nblock_size = 64 # 128\nbatch_size = 16\neval_iters = 200\nmax_iters = 5000\neval_interval = 100\nlearning_rate = 1e-3\ndropout = 0.1\nn_embd = 512\nn_head = 8\nn_layer = 6\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nprint(f\"Train data size: {len(train_data)}\")\nprint(f\"Validation data size: {len(val_data)}\")\nprint(f\"Block size: {block_size}\")\n\n# Data loading function\ndef get_batch(split):\n    data = train_data if split == 'train' else val_data\n    if len(data) <= block_size:\n        raise ValueError(f\"Data length ({len(data)}) must be greater than block size ({block_size}).\")\n\n    max_index = len(data) - block_size\n    ix = torch.randint(0, max_index, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\nclass Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B, T, C = x.shape\n        k = self.key(x)   # (B,T,C)\n        q = self.query(x) # (B,T,C)\n        wei = q @ k.transpose(-2, -1) * C**-0.5 # (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        v = self.value(x) # (B,T,C)\n        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(n_embd, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\nclass FeedForward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedForward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        x = self.blocks(x) # (B,T,C)\n        x = self.ln_f(x) # (B,T,C)\n        logits = self.lm_head(x) # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -block_size:]\n            logits, loss = self(idx_cond)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat((idx, idx_next), dim=1)\n        return idx\n\nmodel = BigramLanguageModel()\nm = model.to(device)\nprint(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    xb, yb = get_batch('train')\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# Generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\ngenerated_text = decode(m.generate(context, max_new_tokens=2000)[0].tolist())\nprint(generated_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T15:06:57.909151Z","iopub.execute_input":"2024-07-25T15:06:57.909503Z","iopub.status.idle":"2024-07-25T15:19:12.491707Z","shell.execute_reply.started":"2024-07-25T15:06:57.909473Z","shell.execute_reply":"2024-07-25T15:19:12.490753Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Train data size: 32477246\nValidation data size: 5731279\nBlock size: 64\n19.07418 M parameters\nstep 0: train loss 4.9077, val loss 4.9073\nstep 100: train loss 2.6259, val loss 2.6202\nstep 200: train loss 2.5119, val loss 2.5002\nstep 300: train loss 2.4180, val loss 2.4135\nstep 400: train loss 2.3601, val loss 2.3565\nstep 500: train loss 2.3073, val loss 2.3143\nstep 600: train loss 2.2707, val loss 2.2931\nstep 700: train loss 2.2447, val loss 2.2588\nstep 800: train loss 2.2187, val loss 2.2220\nstep 900: train loss 2.2003, val loss 2.2127\nstep 1000: train loss 2.1819, val loss 2.1970\nstep 1100: train loss 2.1717, val loss 2.1802\nstep 1200: train loss 2.1564, val loss 2.1732\nstep 1300: train loss 2.1329, val loss 2.1524\nstep 1400: train loss 2.1309, val loss 2.1400\nstep 1500: train loss 2.1336, val loss 2.1385\nstep 1600: train loss 2.1086, val loss 2.1199\nstep 1700: train loss 2.0934, val loss 2.1037\nstep 1800: train loss 2.0974, val loss 2.1075\nstep 1900: train loss 2.0923, val loss 2.1112\nstep 2000: train loss 2.0800, val loss 2.0988\nstep 2100: train loss 2.0685, val loss 2.0883\nstep 2200: train loss 2.0650, val loss 2.0823\nstep 2300: train loss 2.0609, val loss 2.0794\nstep 2400: train loss 2.0491, val loss 2.0703\nstep 2500: train loss 2.0509, val loss 2.0605\nstep 2600: train loss 2.0369, val loss 2.0560\nstep 2700: train loss 2.0408, val loss 2.0550\nstep 2800: train loss 2.0284, val loss 2.0454\nstep 2900: train loss 2.0186, val loss 2.0375\nstep 3000: train loss 2.0194, val loss 2.0472\nstep 3100: train loss 2.0219, val loss 2.0353\nstep 3200: train loss 2.0121, val loss 2.0329\nstep 3300: train loss 2.0135, val loss 2.0302\nstep 3400: train loss 2.0038, val loss 2.0269\nstep 3500: train loss 2.0029, val loss 2.0257\nstep 3600: train loss 1.9998, val loss 2.0246\nstep 3700: train loss 2.0008, val loss 2.0184\nstep 3800: train loss 1.9958, val loss 2.0125\nstep 3900: train loss 1.9958, val loss 2.0066\nstep 4000: train loss 1.9802, val loss 2.0045\nstep 4100: train loss 1.9863, val loss 2.0089\nstep 4200: train loss 1.9798, val loss 1.9996\nstep 4300: train loss 1.9839, val loss 2.0060\nstep 4400: train loss 1.9808, val loss 1.9976\nstep 4500: train loss 1.9658, val loss 1.9946\nstep 4600: train loss 1.9751, val loss 1.9942\nstep 4700: train loss 1.9663, val loss 1.9913\nstep 4800: train loss 1.9658, val loss 1.9920\nstep 4900: train loss 1.9652, val loss 1.9837\nstep 4999: train loss 1.9595, val loss 1.9821\n\tان کسی دراحت که بلای را\nنردنم کرد استی که موجم مباد\nاز زانک نماندمی دل شوی\nکنون خیدم در زین عهد از سحاب\nجمع و اسرادت که سر عشق است\nزعف را حشی تیغ مجلس دهلش\nمقان  جان مدارد چو در عاقل زود همه\nحقایی ز پیاده با مقصود\nبه معنی تو درست تو بدانی\nدر اوچاره زین خاک گل تراندم\nآتش مراگامگه تفعی ستاده ساگر\nمرغ پوشیدن نخورد دو بباشد آواز\nآمد بفش بسید ترت ما نصیع کشید\nخلق بهترمان تو\nهر کلم قوز اسبیده دولت غ شوی\nچو نیک خنجر خوب شو تو به هام\nاگر گشت الایت آنچه سیکی\nبه بر اندر آن ز صفر کشف\nدوتو جهان مردی دریاب انابودش\nماندقدیم کندشه خلخ\nقانی که باشد\nخواب به دادم خوش بر کرد\nنیست ایشان رشت نیست\nدیو جوش گیتی دل مرد از درست\nاو مشک ما گست قاطع و رسید\nکهتری سودای تفرستمت گذاعت\nبا می آزحواد خلق را رکار\nاز عدل و چرخ عقد و رسوق دار\nدست از شگفت و سرانه زند هر جنبشت\nمبر بال دل گر ناگه درگذشت می اما تند\nجنگام نظر فرزند کامیدی جمال\nنمی بی حقیقت از پیدا همی\nبر آنکه کرد دیده ز خداون بند\nالمی بی کن شوق گیسان نطق خود\nعقل بود تو عفکن خود بنده\nچشم به خوب دانه بر فلک را\nببرید گمانی که خاطراب غیش ماه من\nخردمند اسنابان که ادب خوش زشتش با سبوس\nبه مرد در غمزه زلف ادب قدست حقا\nفجله رست گفت ناید نداشت و تعریک\nکفر القیه زنده هیچ آرد\nتا تنت بودی تماشان او به بلا است\nحشل عالم خطی ها هرگیستم که بر دمی داندانش رضولم نکتاب بند\nپریچزدمکاری بدوزخ دوزخت\nچه حضورستم فرد همه برآورد\nترک مضتی عنایی بهر تو باز دگیر\nیوسف به را و توحی تیغ تو\nحصار بیشک و می دهد در درزم\nبورید و خار آمد به محنت یافتع خود\nداری عالم قوی را پدید\nشد و نوش سینه بیو از چوش\nهجرم مکر تو و بر عنبریدارم ازل\nنیک میل و ز آن تویی که ما\nکدامت کند به میبر کل بی آن خسور\nتا نباشد ز منکه که فضل و سعد مرد\nظر انگر چنان از زن گل فروغش به جان\nتا ستاره شد داد گاوی حیر\nدردید هر نقدمه اسم او را خار داد\nاز او جهان ندد زنمک محلق تو\nچو شدمی به قول ز قوت کنم\nشوالست اینجا پرده سر هر خوانی\nکه تا جدایم می راندا فلاک مرا بهشت\nمن بودم در نیک آفتاب آورد\nپس از به خیوفانی یاد همه از ره آفتاب از\nچوگر خدا کز جز روشن کآن دریدش من\nکرد که بگشت به حریم کندمش\nکه ندانجامم نمود که ما هر گرفت\nشه در جان دیگر کاه دو ساحتما\nنبکشته خجل مردم ن رسد نعمت\nسوز زبان که غیر بی دستم\nشد که تو سخت ببنشان\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Choose a model that supports Persian, like mBERT or XLM-RoBERTa\ntokenizer = AutoTokenizer.from_pretrained('aubmindlab/bert-base-arabert')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T15:23:46.075167Z","iopub.execute_input":"2024-07-25T15:23:46.075972Z","iopub.status.idle":"2024-07-25T15:23:47.716612Z","shell.execute_reply.started":"2024-07-25T15:23:46.075941Z","shell.execute_reply":"2024-07-25T15:23:47.715813Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/637 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a753d28c0acc41198ab7249b06d99f8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/578 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5205726789064ee29e658a84441541ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/717k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6d39d4c7fe34daf85d4eb0e7e20a34e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.26M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f2729955fb7430b897752dc5eb82a95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"286a13d9e3144fabadbc7486a4806cf4"}},"metadata":{}}]},{"cell_type":"code","source":"from tokenizers import Tokenizer, models, pre_tokenizers, trainers, normalizers\n\n# Initialize the tokenizer\ntokenizer = Tokenizer(models.BPE())\n\n# Customize pre-tokenization and normalization for Persian text\ntokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\ntokenizer.normalizer = normalizers.NFKC()\n\n# Train the tokenizer on your dataset\ntrainer = trainers.BpeTrainer(vocab_size=50000, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\ntokenizer.train(files=[\"path_to_your_dataset.txt\"], trainer=trainer)\n\n# Save the tokenizer\ntokenizer.save(\"path_to_save_tokenizer/tokenizer.json\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, Trainer, TrainingArguments\n\n# Load a pre-trained model suitable for Persian\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',          \n    num_train_epochs=3,              \n    per_device_train_batch_size=4,  \n    per_device_eval_batch_size=4,   \n    warmup_steps=500,                \n    weight_decay=0.01,              \n    logging_dir='./logs',           \n    logging_steps=10,\n)\n\n# Define the Trainer\ntrainer = Trainer(\n    model=model,                         \n    args=training_args,                  \n    train_dataset=train_data,   \n    eval_dataset=val_data,     \n)\n\n# Train the model\ntrainer.train()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T15:44:35.057721Z","iopub.execute_input":"2024-07-25T15:44:35.058018Z","iopub.status.idle":"2024-07-25T15:47:06.263756Z","shell.execute_reply.started":"2024-07-25T15:44:35.057979Z","shell.execute_reply":"2024-07-25T15:47:06.262073Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.5 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240725_154646-wq0xq1bo</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/fatemehnikan/huggingface/runs/wq0xq1bo' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/fatemehnikan/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/fatemehnikan/huggingface' target=\"_blank\">https://wandb.ai/fatemehnikan/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/fatemehnikan/huggingface/runs/wq0xq1bo' target=\"_blank\">https://wandb.ai/fatemehnikan/huggingface/runs/wq0xq1bo</a>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 27\u001b[0m\n\u001b[1;32m     19\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     20\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,                         \n\u001b[1;32m     21\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,                  \n\u001b[1;32m     22\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_data,   \n\u001b[1;32m     23\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mval_data,     \n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1932\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1930\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1933\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1937\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2230\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2227\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   2231\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39minclude_num_input_tokens_seen:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/data_loader.py:454\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 454\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:620\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter._next_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_index\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 620\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sampler_iter\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/sampler.py:282\u001b[0m, in \u001b[0;36mBatchSampler.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m batch \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[1;32m    281\u001b[0m idx_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampler:\n\u001b[1;32m    283\u001b[0m     batch[idx_in_batch] \u001b[38;5;241m=\u001b[39m idx\n\u001b[1;32m    284\u001b[0m     idx_in_batch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/data_loader.py:92\u001b[0m, in \u001b[0;36mSeedableRandomSampler.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# print(\"Setting seed at epoch\", self.epoch, seed)\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[0;32m---> 92\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_epoch(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/sampler.py:164\u001b[0m, in \u001b[0;36mRandomSampler.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n):\n\u001b[0;32m--> 164\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandperm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mrandperm(n, generator\u001b[38;5;241m=\u001b[39mgenerator)\u001b[38;5;241m.\u001b[39mtolist()[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m%\u001b[39m n]\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"import os\nimport math\nimport time\nimport inspect\nfrom dataclasses import dataclass\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport glob\n# from hellaswag import render_example, iterate_examples\n# -----------------------------------------------------------------------------\n\nclass CausalSelfAttention(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n        # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n        self.c_proj.NANOGPT_SCALE_INIT = 1\n        # regularization\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n        qkv = self.c_attn(x)\n        q, k, v = qkv.split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # flash attention\n        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n        # output projection\n        y = self.c_proj(y)\n        return y\n\nclass MLP(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n        self.gelu    = nn.GELU(approximate='tanh')\n        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n        self.c_proj.NANOGPT_SCALE_INIT = 1\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = self.gelu(x)\n        x = self.c_proj(x)\n        return x\n\nclass Block(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.ln_1 = nn.LayerNorm(config.n_embd)\n        self.attn = CausalSelfAttention(config)\n        self.ln_2 = nn.LayerNorm(config.n_embd)\n        self.mlp = MLP(config)\n\n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\n\n@dataclass\nclass GPTConfig:\n    block_size: int = 1024 # max sequence length\n    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n    n_layer: int = 12 # number of layers\n    n_head: int = 12 # number of heads\n    n_embd: int = 768 # embedding dimension\n\nclass GPT(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n\n        self.transformer = nn.ModuleDict(dict(\n            wte = nn.Embedding(config.vocab_size, config.n_embd),\n            wpe = nn.Embedding(config.block_size, config.n_embd),\n            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            ln_f = nn.LayerNorm(config.n_embd),\n        ))\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n\n        # weight sharing scheme\n        self.transformer.wte.weight = self.lm_head.weight\n\n        # init params\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            std = 0.02\n            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n                std *= (2 * self.config.n_layer) ** -0.5\n            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None):\n        # idx is of shape (B, T)\n        B, T = idx.size()\n        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n        # forward the token and posisition embeddings\n        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n        x = tok_emb + pos_emb\n        # forward the blocks of the transformer\n        for block in self.transformer.h:\n            x = block(x)\n        # forward the final layernorm and the classifier\n        x = self.transformer.ln_f(x)\n        logits = self.lm_head(x) # (B, T, vocab_size)\n        loss = None\n        if targets is not None:\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n        return logits, loss\n\n    @classmethod\n    def from_pretrained(cls, model_type):\n        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n        from transformers import GPT2LMHeadModel\n        print(\"loading weights from pretrained gpt: %s\" % model_type)\n\n        # n_layer, n_head and n_embd are determined from model_type\n        config_args = {\n            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n        }[model_type]\n        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n        # create a from-scratch initialized minGPT model\n        config = GPTConfig(**config_args)\n        model = GPT(config)\n        sd = model.state_dict()\n        sd_keys = sd.keys()\n        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n\n        # init a huggingface/transformers model\n        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n        sd_hf = model_hf.state_dict()\n\n        # copy while ensuring all of the parameters are aligned and match in names and shapes\n        sd_keys_hf = sd_hf.keys()\n        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n        # this means that we have to transpose these weights when we import them\n        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n        for k in sd_keys_hf:\n            if any(k.endswith(w) for w in transposed):\n                # special treatment for the Conv1D weights we need to transpose\n                assert sd_hf[k].shape[::-1] == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k].t())\n            else:\n                # vanilla copy over the other parameters\n                assert sd_hf[k].shape == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k])\n\n        return model\n\n    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n        # start with all of the candidate parameters (that require grad)\n        param_dict = {pn: p for pn, p in self.named_parameters()}\n        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n        optim_groups = [\n            {'params': decay_params, 'weight_decay': weight_decay},\n            {'params': nodecay_params, 'weight_decay': 0.0}\n        ]\n        num_decay_params = sum(p.numel() for p in decay_params)\n        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n        if master_process:\n            print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n            print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n        # Create AdamW optimizer and use the fused version if it is available\n        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n        use_fused = fused_available and device_type == \"cuda\"\n        if master_process:\n            print(f\"using fused AdamW: {use_fused}\")\n        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n        return optimizer\n\n# -----------------------------------------------------------------------------\nimport tiktoken\nimport numpy as np\n\ndef load_tokens(filename):\n    npt = np.load(filename)\n    npt = npt.astype(np.int32) # added after video\n    ptt = torch.tensor(npt, dtype=torch.long)\n    return ptt\n\nclass DataLoaderLite:\n    def __init__(self, B, T, process_rank, num_processes, split):\n        self.B = B\n        self.T = T\n        self.process_rank = process_rank\n        self.num_processes = num_processes\n        assert split in {'train', 'val'}\n        \n        # Load the text files\n        directory = '/kaggle/input/poems-dataset/'\n        text_files = glob.glob(os.path.join(directory, '*.txt'))\n        text = []\n        for file_path in text_files:\n            try:\n                with open(file_path, 'r', encoding='utf-8') as file:\n                    content = file.read()\n                    text.append(content)\n            except UnicodeDecodeError as e:\n                print(f\"Skipping file {file_path} due to encoding error: {e}\")\n\n        # Concatenate all the text content into a single string\n        text = ''.join(text)\n        \n        # get the shard filenames\n        data_root =  text #\"edu_fineweb10B\"\n        shards = os.listdir(data_root)\n        shards = [s for s in shards if split in s]\n        shards = sorted(shards)\n        shards = [os.path.join(data_root, s) for s in shards]\n        self.shards = shards\n        assert len(shards) > 0, f\"no shards found for split {split}\"\n        if master_process:\n            print(f\"found {len(shards)} shards for split {split}\")\n        self.reset()\n\n    def reset(self):\n        # state, init at shard zero\n        self.current_shard = 0\n        self.tokens = load_tokens(self.shards[self.current_shard])\n        self.current_position = self.B * self.T * self.process_rank\n\n    def next_batch(self):\n        B, T = self.B, self.T\n        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n        x = (buf[:-1]).view(B, T) # inputs\n        y = (buf[1:]).view(B, T) # targets\n        # advance the position in the tensor\n        self.current_position += B * T * self.num_processes\n        # if loading the next batch would be out of bounds, advance to next shard\n        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n            self.current_shard = (self.current_shard + 1) % len(self.shards)\n            self.tokens = load_tokens(self.shards[self.current_shard])\n            self.current_position = B * T * self.process_rank\n        return x, y\n\n# -----------------------------------------------------------------------------\n# helper function for HellaSwag eval\n# takes tokens, mask, and logits, returns the index of the completion with the lowest loss\n\ndef get_most_likely_row(tokens, mask, logits):\n    # evaluate the autoregressive loss at all positions\n    shift_logits = (logits[..., :-1, :]).contiguous()\n    shift_tokens = (tokens[..., 1:]).contiguous()\n    flat_shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n    flat_shift_tokens = shift_tokens.view(-1)\n    shift_losses = F.cross_entropy(flat_shift_logits, flat_shift_tokens, reduction='none')\n    shift_losses = shift_losses.view(tokens.size(0), -1)\n    # now get the average loss just for the completion region (where mask == 1), in each row\n    shift_mask = (mask[..., 1:]).contiguous() # we must shift mask, so we start at the last prompt token\n    masked_shift_losses = shift_losses * shift_mask\n    # sum and divide by the number of 1s in the mask\n    sum_loss = masked_shift_losses.sum(dim=1)\n    avg_loss = sum_loss / shift_mask.sum(dim=1)\n    # now we have a loss for each of the 4 completions\n    # the one with the lowest loss should be the most likely\n    pred_norm = avg_loss.argmin().item()\n    return pred_norm\n\n# -----------------------------------------------------------------------------\n# simple launch:\n# python train_gpt2.py\n# DDP launch for e.g. 8 GPUs:\n# torchrun --standalone --nproc_per_node=8 train_gpt2.py\n\n# run the training loop\nfrom torch.distributed import init_process_group, destroy_process_group\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nimport torch.distributed as dist\n\n# set up DDP (distributed data parallel).\n# torchrun command sets the env variables RANK, LOCAL_RANK, and WORLD_SIZE\nddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\nif ddp:\n    # use of DDP atm demands CUDA, we set the device appropriately according to rank\n    assert torch.cuda.is_available(), \"for now i think we need CUDA for DDP\"\n    init_process_group(backend='nccl')\n    ddp_rank = int(os.environ['RANK'])\n    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n    ddp_world_size = int(os.environ['WORLD_SIZE'])\n    device = f'cuda:{ddp_local_rank}'\n    torch.cuda.set_device(device)\n    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\nelse:\n    # vanilla, non-DDP run\n    ddp_rank = 0\n    ddp_local_rank = 0\n    ddp_world_size = 1\n    master_process = True\n    # attempt to autodetect device\n    device = \"cpu\"\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n        device = \"mps\"\n    print(f\"using device: {device}\")\n\n# added after video, pytorch can be serious about it's device vs. device_type distinction\ndevice_type = \"cuda\" if device.startswith(\"cuda\") else \"cpu\"\n\ntorch.manual_seed(1337)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(1337)\n\nenc = tiktoken.get_encoding(\"gpt2\")\n\ntotal_batch_size = 524288 # 2**19, ~0.5M, in number of tokens\nB = 64 # micro batch size\nT = 1024 # sequence length\nassert total_batch_size % (B * T * ddp_world_size) == 0, \"make sure total_batch_size is divisible by B * T * ddp_world_size\"\ngrad_accum_steps = total_batch_size // (B * T * ddp_world_size)\nif master_process:\n    print(f\"total desired batch size: {total_batch_size}\")\n    print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n\ntrain_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, split=\"train\")\nval_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, split=\"val\")\n\ntorch.set_float32_matmul_precision('high')\n\n# create model\nmodel = GPT(GPTConfig(vocab_size=50304))\n# model = GPT.from_pretrained(\"gpt2\") # or init from OpenAI GPT-2\nmodel.to(device)\nuse_compile = False # torch.compile interferes with HellaSwag eval and Generation. TODO fix\nif use_compile:\n    model = torch.compile(model)\nif ddp:\n    model = DDP(model, device_ids=[ddp_local_rank])\nraw_model = model.module if ddp else model # always contains the \"raw\" unwrapped model\n\nmax_lr = 6e-4\nmin_lr = max_lr * 0.1\nwarmup_steps = 715\nmax_steps = 19073 # 19,073 steps is ~1 epoch, if data is 10B tokens and batch size 0.5M tokens\ndef get_lr(it):\n    # 1) linear warmup for warmup_iters steps\n    if it < warmup_steps:\n        return max_lr * (it+1) / warmup_steps\n    # 2) if it > lr_decay_iters, return min learning rate\n    if it > max_steps:\n        return min_lr\n    # 3) in between, use cosine decay down to min learning rate\n    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n    assert 0 <= decay_ratio <= 1\n    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n    return min_lr + coeff * (max_lr - min_lr)\n\n# optimize!\noptimizer = raw_model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device_type=device_type)\n\n# create the log directory we will write checkpoints to and log to\nlog_dir = \"log\"\nos.makedirs(log_dir, exist_ok=True)\nlog_file = os.path.join(log_dir, f\"log.txt\")\nwith open(log_file, \"w\") as f: # open for writing to clear the file\n    pass\n\nfor step in range(max_steps):\n    t0 = time.time()\n    last_step = (step == max_steps - 1)\n\n    # once in a while evaluate our validation loss\n    if step % 250 == 0 or last_step:\n        model.eval()\n        val_loader.reset()\n        with torch.no_grad():\n            val_loss_accum = 0.0\n            val_loss_steps = 20\n            for _ in range(val_loss_steps):\n                x, y = val_loader.next_batch()\n                x, y = x.to(device), y.to(device)\n                with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n                    logits, loss = model(x, y)\n                loss = loss / val_loss_steps\n                val_loss_accum += loss.detach()\n        if ddp:\n            dist.all_reduce(val_loss_accum, op=dist.ReduceOp.AVG)\n        if master_process:\n            print(f\"validation loss: {val_loss_accum.item():.4f}\")\n            with open(log_file, \"a\") as f:\n                f.write(f\"{step} val {val_loss_accum.item():.4f}\\n\")\n            if step > 0 and (step % 5000 == 0 or last_step):\n                # optionally write model checkpoints\n                checkpoint_path = os.path.join(log_dir, f\"model_{step:05d}.pt\")\n                checkpoint = {\n                    'model': raw_model.state_dict(),\n                    'config': raw_model.config,\n                    'step': step,\n                    'val_loss': val_loss_accum.item()\n                }\n                # you might also want to add optimizer.state_dict() and\n                # rng seeds etc., if you wanted to more exactly resume training\n                torch.save(checkpoint, checkpoint_path)\n\n    # once in a while evaluate hellaswag\n    # if (step % 250 == 0 or last_step) and (not use_compile):\n    #     num_correct_norm = 0\n    #     num_total = 0\n    #     for i, example in enumerate(iterate_examples(\"val\")):\n    #         # only process examples where i % ddp_world_size == ddp_rank\n    #         if i % ddp_world_size != ddp_rank:\n    #             continue\n    #         # render the example into tokens and labels\n    #         _, tokens, mask, label = render_example(example)\n    #         tokens = tokens.to(device)\n    #         mask = mask.to(device)\n    #         # get the logits\n    #         with torch.no_grad():\n    #             with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n    #                 logits, loss = model(tokens)\n    #             pred_norm = get_most_likely_row(tokens, mask, logits)\n    #         num_total += 1\n    #         num_correct_norm += int(pred_norm == label)\n    #     # reduce the stats across all processes\n    #     if ddp:\n    #         num_total = torch.tensor(num_total, dtype=torch.long, device=device)\n    #         num_correct_norm = torch.tensor(num_correct_norm, dtype=torch.long, device=device)\n    #         dist.all_reduce(num_total, op=dist.ReduceOp.SUM)\n    #         dist.all_reduce(num_correct_norm, op=dist.ReduceOp.SUM)\n    #         num_total = num_total.item()\n    #         num_correct_norm = num_correct_norm.item()\n    #     acc_norm = num_correct_norm / num_total\n    #     if master_process:\n    #         print(f\"HellaSwag accuracy: {num_correct_norm}/{num_total}={acc_norm:.4f}\")\n    #         with open(log_file, \"a\") as f:\n    #             f.write(f\"{step} hella {acc_norm:.4f}\\n\")\n\n    # once in a while generate from the model (except step 0, which is noise)\n    if ((step > 0 and step % 250 == 0) or last_step) and (not use_compile):\n        model.eval()\n        num_return_sequences = 4\n        max_length = 32\n        # tokens = enc.encode(\"Hello, I'm a language model,\")\n        tokens = enc.encode(\"توانا بود آنکه دانا بود\")\n        tokens = torch.tensor(tokens, dtype=torch.long)\n        tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n        xgen = tokens.to(device)\n        sample_rng = torch.Generator(device=device)\n        sample_rng.manual_seed(42 + ddp_rank)\n        while xgen.size(1) < max_length:\n            # forward the model to get the logits\n            with torch.no_grad():\n                with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n                    logits, loss = model(xgen) # (B, T, vocab_size)\n                # take the logits at the last position\n                logits = logits[:, -1, :] # (B, vocab_size)\n                # get the probabilities\n                probs = F.softmax(logits, dim=-1)\n                # do top-k sampling of 50 (huggingface pipeline default)\n                # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n                topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n                # select a token from the top-k probabilities\n                # note: multinomial does not demand the input to sum to 1\n                ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n                # gather the corresponding indices\n                xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n                # append to the sequence\n                xgen = torch.cat((xgen, xcol), dim=1)\n        # print the generated text\n        for i in range(num_return_sequences):\n            tokens = xgen[i, :max_length].tolist()\n            decoded = enc.decode(tokens)\n            print(f\"rank {ddp_rank} sample {i}: {decoded}\")\n\n    # do one step of the optimization\n    model.train()\n    optimizer.zero_grad()\n    loss_accum = 0.0\n    for micro_step in range(grad_accum_steps):\n        x, y = train_loader.next_batch()\n        x, y = x.to(device), y.to(device)\n        # added after video, this field is also used by the forward pass.\n        if ddp:\n            model.require_backward_grad_sync = (micro_step == grad_accum_steps - 1)\n        with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n            logits, loss = model(x, y)\n        # we have to scale the loss to account for gradient accumulation,\n        # because the gradients just add on each successive backward().\n        # addition of gradients corresponds to a SUM in the objective, but\n        # instead of a SUM we want MEAN. Scale the loss here so it comes out right\n        loss = loss / grad_accum_steps\n        loss_accum += loss.detach()\n        loss.backward()\n    if ddp:\n        dist.all_reduce(loss_accum, op=dist.ReduceOp.AVG)\n    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n    # determine and set the learning rate for this iteration\n    lr = get_lr(step)\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n    optimizer.step()\n    if device_type == \"cuda\":\n        torch.cuda.synchronize() # wait for the GPU to finish work\n    t1 = time.time()\n    dt = t1 - t0 # time difference in seconds\n    tokens_processed = train_loader.B * train_loader.T * grad_accum_steps * ddp_world_size\n    tokens_per_sec = tokens_processed / dt\n    if master_process:\n        print(f\"step {step:5d} | loss: {loss_accum.item():.6f} | lr {lr:.4e} | norm: {norm:.4f} | dt: {dt*1000:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")\n        with open(log_file, \"a\") as f:\n            f.write(f\"{step} train {loss_accum.item():.6f}\\n\")\n\nif ddp:\n    destroy_process_group()","metadata":{"execution":{"iopub.status.busy":"2024-07-25T18:44:20.922217Z","iopub.execute_input":"2024-07-25T18:44:20.923195Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"using device: cuda\ntotal desired batch size: 524288\n=> calculated gradient accumulation steps: 8\n","output_type":"stream"}]},{"cell_type":"code","source":"text = 'توانا بود هرکه دانا بود'","metadata":{"execution":{"iopub.status.busy":"2024-07-25T18:27:11.586447Z","iopub.execute_input":"2024-07-25T18:27:11.586896Z","iopub.status.idle":"2024-07-25T18:27:11.598234Z","shell.execute_reply.started":"2024-07-25T18:27:11.586867Z","shell.execute_reply":"2024-07-25T18:27:11.597093Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-multilingual-cased\")\n\ntokenizer(text, return_tensors=\"pt\")\n\ntokenizer.decode( [  101, 10289, 11330,   788, 71766,   782, 31951,   770, 21046, 10700,102])","metadata":{"execution":{"iopub.status.busy":"2024-07-25T18:27:19.224487Z","iopub.execute_input":"2024-07-25T18:27:19.224924Z","iopub.status.idle":"2024-07-25T18:27:20.223836Z","shell.execute_reply.started":"2024-07-25T18:27:19.224895Z","shell.execute_reply":"2024-07-25T18:27:20.222696Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97f251bccfb645a2b0c148bbfc616005"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7599a6f1ef884ad594623d505d71f321"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"096f53e248dc44f6b69e16ddf12c037f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"333a001a54f54185aeb1787c8b2b8792"}},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"'[CLS] من آن مرغ غزل خوانم [SEP]'"},"metadata":{}}]},{"cell_type":"code","source":"from minbpe import BasicTokenizer\ntokenizer = BasicTokenizer()\ntokenizer.train(very_long_training_string, vocab_size=4096)\ntokenizer.encode(\"hello world\") # string -> tokens\ntokenizer.decode([1000, 2000, 3000]) # tokens -> string\ntokenizer.save(\"mymodel\") # writes mymodel.model and mymodel.vocab\ntokenizer.load(\"mymodel.model\") # loads the model back, the vocab is just for vis","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import glob\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport numpy as np\n\nfrom transformers import (\n    AutoTokenizer,\n    TextDataset,\n    DataCollatorForLanguageModeling,\n    Trainer,\n    TrainingArguments,\n    AutoModelWithLMHead,\n)\ntorch.manual_seed(1337)\n\n# Load the text files\ndirectory = '/content/drive/MyDrive/Poems/'\ntext_files = glob.glob(os.path.join(directory, '*.txt'))\ntext = []\nfor file_path in text_files:\n    try:\n        with open(file_path, 'r', encoding='utf-8') as file:\n            content = file.read()\n            text.append(content)\n    except UnicodeDecodeError as e:\n        print(f\"Skipping file {file_path} due to encoding error: {e}\")\n\n# Concatenate all the text content into a single string\ntext = ' '.join(text)\n\n# tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-multilingual-cased\")\ntokenizer = AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian')\n# tokenized_inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\ntokenized_inputs = tokenizer.encode(text,padding = True)\ntokenized_np = np.array(tokenized_inputs, dtype= np.uint16)\n# Create a character-level vocabulary\n# chars = sorted(list(set(text)))\n# vocab_size = len(chars)\n# # Create a mapping from characters to integers\n# stoi = { ch:i for i,ch in enumerate(chars) }\n# itos = { i:ch for i,ch in enumerate(chars) }\n# encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n# decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\n# Train and test splits\n# data = torch.tensor(encode(text), dtype=torch.long)\n# data = tokenized_inputs['input_ids']\ndata = tokenized_np\nn = int(0.85 * len(data))\ntrain_data = data[:n]\nval_data = data[n:]\n\n# train_data, val_data, data_collator = load_dataset(train_path, test_path, tokenizer)\n\n# Ensure block size is less than data size\n# vocab_size = 50257\nvocab_size = tokenizer.vocab_size\nblock_size = 64 # 128\nbatch_size = 16\neval_iters = 200\nmax_iters = 5000\neval_interval = 100\nlearning_rate = 1e-3\ndropout = 0.1\nn_embd = 512\nn_head = 8\nn_layer = 6\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nprint(f\"Train data size: {len(train_data)}\")\nprint(f\"Validation data size: {len(val_data)}\")\nprint(f\"Block size: {block_size}\")\n\n# Data loading function\ndef get_batch(split):\n    data = train_data if split == 'train' else val_data\n    if len(data) <= block_size:\n        raise ValueError(f\"Data length ({len(data)}) must be greater than block size ({block_size}).\")\n    max_index = len(data) - block_size\n    ix = torch.randint(0, max_index, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\nclass Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B, T, C = x.shape\n        k = self.key(x)   # (B,T,C)\n        q = self.query(x) # (B,T,C)\n        wei = q @ k.transpose(-2, -1) * C**-0.5 # (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        v = self.value(x) # (B,T,C)\n        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(n_embd, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\nclass FeedForward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedForward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        x = self.blocks(x) # (B,T,C)\n        x = self.ln_f(x) # (B,T,C)\n        logits = self.lm_head(x) # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -block_size:]\n            logits, loss = self(idx_cond)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat((idx, idx_next), dim=1)\n        return idx\n\nmodel = BigramLanguageModel()\nm = model.to(device)\nprint(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    xb, yb = get_batch('train')\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# Generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\ngenerated_text = decode(m.generate(context, max_new_tokens=2000)[0].tolist())\nprint(generated_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-27T17:03:03.432573Z","iopub.execute_input":"2024-07-27T17:03:03.433030Z","iopub.status.idle":"2024-07-27T17:03:05.328327Z","shell.execute_reply.started":"2024-07-27T17:03:03.432989Z","shell.execute_reply":"2024-07-27T17:03:05.326892Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Train data size: 1\nValidation data size: 1\nBlock size: 64\n44.56388 M parameters\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 219\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iters):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m%\u001b[39m eval_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m==\u001b[39m max_iters \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         losses \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    222\u001b[0m     xb, yb \u001b[38;5;241m=\u001b[39m get_batch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[2], line 97\u001b[0m, in \u001b[0;36mestimate_loss\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m losses \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(eval_iters)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(eval_iters):\n\u001b[0;32m---> 97\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m model(X, Y)\n\u001b[1;32m     99\u001b[0m     losses[k] \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n","Cell \u001b[0;32mIn[2], line 82\u001b[0m, in \u001b[0;36mget_batch\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m     80\u001b[0m data \u001b[38;5;241m=\u001b[39m train_data \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m val_data\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m block_size:\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData length (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be greater than block size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblock_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     83\u001b[0m max_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m-\u001b[39m block_size\n\u001b[1;32m     84\u001b[0m ix \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, max_index, (batch_size,))\n","\u001b[0;31mValueError\u001b[0m: Data length (1) must be greater than block size (64)."],"ename":"ValueError","evalue":"Data length (1) must be greater than block size (64).","output_type":"error"}]}]}